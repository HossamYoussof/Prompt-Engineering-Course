{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DEAKbfqmGK7"
      },
      "source": [
        "# **HuggingFace Models**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-c_lGHDJXuk"
      },
      "source": [
        "## installation\n",
        "https://python.langchain.com/docs/integrations/chat/huggingface/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56V5zok8sT8q"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "import getpass\n",
        "import os\n",
        "\n",
        "# https://huggingface.co/settings/tokens\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]=userdata.get('huggingToken')\n",
        "\n",
        "if \"HUGGINGFACEHUB_API_TOKEN\" not in os.environ:\n",
        "    os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = getpass.getpass(\"Enter your Hugging Face API key: \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g4-CYHkVr-kB"
      },
      "outputs": [],
      "source": [
        "%pip install --upgrade --quiet langchain-huggingface"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uz6jdc9JJeUy"
      },
      "source": [
        "## huggingface Endpoint\n",
        "\n",
        "https://python.langchain.com/docs/integrations/llms/huggingface_endpoint/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "repo_id = \"microsoft/Phi-3.5-mini-instruct\"\n",
        "\n",
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id=repo_id,\n",
        "    task=\"text-generation\",\n",
        "    max_length=128,\n",
        "    temperature=0.5,\n",
        ")"
      ],
      "metadata": {
        "id": "IpTj7cec_i3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nFUDQRQEyaF-"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "template = \"explain {subject} i am 5\"\n",
        "\n",
        "prompt_template = PromptTemplate(template=template, input_variables=[\"subject\"])\n",
        "\n",
        "\n",
        "user_subject = \"Machine Learning\"\n",
        "\n",
        "prompt = prompt_template.format(subject=user_subject)\n",
        "print(\"prompt: \" + prompt)\n",
        "\n",
        "ai_msg=llm.invoke(prompt)\n",
        "print(\"answer: \" + ai_msg)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"explain {subject} i am 5, Write the answer in {language}\"\n",
        "\n",
        "prompt_template = PromptTemplate(\n",
        "    template=template,\n",
        "    input_variables=[\"subject\", \"language\"]\n",
        ")\n",
        "\n",
        "user_subject = \"Machine Learning\"\n",
        "user_language = \"Arabic\"\n",
        "\n",
        "prompt = prompt_template.format(subject=user_subject, language=user_language)\n",
        "print(\"prompt: \" + prompt)\n",
        "\n",
        "ai_msg=llm.invoke(prompt)\n",
        "print(\"answer: \" + ai_msg)"
      ],
      "metadata": {
        "id": "1v9YmK4fhXj4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\\n\".join([\n",
        "    \"اشرح لي\",\n",
        "    \"{subject}\",\n",
        "    \"كما لو أنني أبلغ الخامسة من العمر\"\n",
        "])\n",
        "\n",
        "prompt_template = PromptTemplate(\n",
        "    template=template,\n",
        "    input_variables=[\"subject\"]\n",
        ")\n",
        "\n",
        "user_subject = \"الهرم الغذائي\"\n",
        "\n",
        "prompt = prompt_template.format(subject=user_subject)\n",
        "print(\"prompt: \" + prompt)\n",
        "\n",
        "llm_answer=llm.invoke(prompt)\n",
        "print(\"answer: \" + llm_answer)"
      ],
      "metadata": {
        "id": "B8sHOc0ahvfR"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}