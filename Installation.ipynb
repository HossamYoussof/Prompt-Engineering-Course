{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYmisq1Klw-m"
      },
      "source": [
        "# **GPU info**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J65KYam3ok16"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiX1OoEIsY7a"
      },
      "source": [
        "# **OpenAI**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syJ59rWUskSw"
      },
      "source": [
        "## Installation\n",
        "\n",
        "https://python.langchain.com/docs/integrations/chat/openai/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8bqLH9tjspOa"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "import getpass\n",
        "import os\n",
        "\n",
        "# https://platform.openai.com/api-keys\n",
        "os.environ[\"OPENAI_API_KEY\"]=userdata.get('openaiKey')\n",
        "\n",
        "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pQp2CUMPtMYf"
      },
      "outputs": [],
      "source": [
        "%pip install -qU langchain-openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnNNcnPRttRX"
      },
      "source": [
        "## gpt-4o lllm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JG7UZYndt43-"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    model=\"gpt-4o\",\n",
        "    temperature=0,\n",
        "    max_tokens=None,\n",
        "    timeout=None,\n",
        "    max_retries=2,\n",
        "    # api_key=\"...\",  # if you prefer to pass api key in directly instaed of using env vars\n",
        "    # base_url=\"...\",\n",
        "    # organization=\"...\",\n",
        "    # other params...\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d3UBdIgDt6WF"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    (\n",
        "        \"system\",\n",
        "        \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\n",
        "    ),\n",
        "    (\"human\", \"I love programming.\"),\n",
        "]\n",
        "ai_msg = llm.invoke(messages)\n",
        "ai_msg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SW8ULQbWl9W3"
      },
      "source": [
        "# **Mistral**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMHngJ3-IY2P"
      },
      "source": [
        "## Installation\n",
        "https://python.langchain.com/docs/integrations/chat/mistralai/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KMHH_a3psQu-"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "import getpass\n",
        "import os\n",
        "\n",
        "# https://console.mistral.ai/api-keys/\n",
        "os.environ[\"MISTRAL_API_KEY\"]=userdata.get('mistralKey')\n",
        "\n",
        "if \"MISTRAL_API_KEY\" not in os.environ:\n",
        "    os.environ[\"MISTRAL_API_KEY\"] = getpass.getpass(\"Enter your Mistral API key: \")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPy34JxprYS9"
      },
      "outputs": [],
      "source": [
        "%pip install -qU langchain_mistralai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGTJK_HPI_P2"
      },
      "source": [
        "## mistral llm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gKuCdzFnXFyz"
      },
      "outputs": [],
      "source": [
        "from langchain_mistralai import ChatMistralAI\n",
        "\n",
        "llm = ChatMistralAI(\n",
        "    model=\"mistral-large-latest\",\n",
        "    temperature=0,\n",
        "    max_retries=2,\n",
        "    # other params...\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uAsAsc6aXM5D"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    (\n",
        "        \"system\",\n",
        "        \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\n",
        "    ),\n",
        "    (\"human\", \"I love programming.\"),\n",
        "]\n",
        "ai_msg = llm.invoke(messages)\n",
        "ai_msg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DEAKbfqmGK7"
      },
      "source": [
        "# **HuggingFace Models**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-c_lGHDJXuk"
      },
      "source": [
        "## installation\n",
        "https://python.langchain.com/docs/integrations/chat/huggingface/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56V5zok8sT8q"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "import getpass\n",
        "import os\n",
        "\n",
        "# https://huggingface.co/settings/tokens\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]=userdata.get('huggingToken')\n",
        "\n",
        "if \"HUGGINGFACEHUB_API_TOKEN\" not in os.environ:\n",
        "    os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = getpass.getpass(\"Enter your Hugging Face API key: \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g4-CYHkVr-kB"
      },
      "outputs": [],
      "source": [
        "%pip install --upgrade --quiet  langchain-huggingface text-generation transformers google-search-results numexpr langchainhub sentencepiece jinja2 bitsandbytes accelerate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uz6jdc9JJeUy"
      },
      "source": [
        "## huggingface endpoint llm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lI23SAyOu5MM"
      },
      "outputs": [],
      "source": [
        "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
        "\n",
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id=\"HuggingFaceH4/zephyr-7b-beta\",\n",
        "    task=\"text-generation\",\n",
        "    max_new_tokens=512,\n",
        "    do_sample=False,\n",
        "    repetition_penalty=1.03,\n",
        ")\n",
        "\n",
        "chat_model = ChatHuggingFace(llm=llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nFUDQRQEyaF-"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import (\n",
        "    HumanMessage,\n",
        "    SystemMessage,\n",
        ")\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"You're a helpful assistant\"),\n",
        "    HumanMessage(\n",
        "        content=\"What happens when an unstoppable force meets an immovable object?\"\n",
        "    ),\n",
        "]\n",
        "\n",
        "ai_msg = chat_model.invoke(messages)\n",
        "ai_msg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgdqKXonJoI3"
      },
      "source": [
        "## huggingface pipeline llm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UdQyvV0Izu1W"
      },
      "outputs": [],
      "source": [
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=\"float16\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lEWT-8xp0grT"
      },
      "outputs": [],
      "source": [
        "from langchain_huggingface import HuggingFacePipeline\n",
        "\n",
        "llm = HuggingFacePipeline.from_model_id(\n",
        "    model_id=\"HuggingFaceH4/zephyr-7b-beta\",\n",
        "    task=\"text-generation\",\n",
        "    pipeline_kwargs=dict(\n",
        "        max_new_tokens=512,\n",
        "        do_sample=False,\n",
        "        repetition_penalty=1.03,\n",
        "        return_full_text=False,\n",
        "    ),\n",
        "    model_kwargs={\"quantization_config\": quantization_config},\n",
        ")\n",
        "\n",
        "chat_model = ChatHuggingFace(llm=llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mBbKgKsl05nU"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import (\n",
        "    HumanMessage,\n",
        "    SystemMessage,\n",
        ")\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"You're a helpful assistant\"),\n",
        "    HumanMessage(\n",
        "        content=\"What happens when an unstoppable force meets an immovable object?\"\n",
        "    ),\n",
        "]\n",
        "\n",
        "ai_msg = chat_model.invoke(messages)\n",
        "ai_msg"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ml",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
